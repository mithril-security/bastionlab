{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune DistilBERT for binary classification on the SMS Spam Collection\n",
    "__________________________\n",
    "\n",
    "***# This a lot to not say much ^^ We need a real life example specific to DistilBERT doing binary classification on the SMS Spam collection***\n",
    "\n",
    "This can be useful, for instance, when one wants to leverage large pre-trained models on a smaller private dataset, for instance, medical or financial records, and ensure data privacy regarding users' data.\n",
    "\n",
    "BastionLabTorch is intended for scenarios where we have a data owner, for instance, a hospital, wanting to have third parties train models on their data, e.g. a startup, potentially on untrusted infrastructures, such as in the Cloud.\n",
    "\n",
    "The strength of BastionLabTorch is that the data owner can have a high level of protection on data shared to a remote enclave hosted in the Cloud, and operated by the startup, thanks to memory isolation and encryption, and remote attestation from the use of secure enclaves. \n",
    "\n",
    "***# end comment***\n",
    "\n",
    "In this notebook, we will illustrate how BastionLab works. We will use the publicly available dataset [SMS Spam Collection](https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset) to finetune a DistilBERT model on a classification task, to predict whether an email is spam or not.\n",
    "\n",
    "***# Why the SPAM Collection dataset?***\n",
    "\n",
    "In this guide, we will cover two phases:\n",
    "- The offline phase, in which the data owner prepares the dataset and the data scientist prepares the model.\n",
    "- The online phase, in which dataset and model are uploaded to the secure enclave. In the enclave, the uploaded model will be trained on the dataset. The data scientist can pull the weights once the training is over.\n",
    "\n",
    "We largely followed [this tutorial](https://towardsdatascience.com/fine-tuning-bert-for-text-classification-54e7df642894) to prepare the data and train the model in this example.\n",
    "\n",
    "***# What do you mean by largely based?***\n",
    "\n",
    "\n",
    "## Pre-requisites\n",
    "__________________________\n",
    "\n",
    "We need to have installed: \n",
    "- [BastionLab](https://bastionlab.readthedocs.io/en/latest/docs/getting-started/installation/)\n",
    "- Hugging Face's [Transformers library](https://huggingface.co/docs/transformers/installation)\n",
    "- [Pandas](https://pandas.pydata.org/getting_started.html)\n",
    "- [IPython kernel](https://ipython.readthedocs.io/en/stable/install/kernel_install.html) for Jupyter\n",
    "- [Jupyter Widgets](https://ipywidgets.readthedocs.io/en/7.x/user_install.html) to enable notebooks extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bastionlab\n",
    "!pip install transformers pandas ipykernel ipywidgets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offline phase - Model and dataset preparation\n",
    "__________________________________\n",
    "\n",
    "### Data owner's side: preparing the dataset\n",
    "\n",
    "***# When do you suppose we might need that? Is there applied examples? Why would we have a data owner wanting to do that?***\n",
    "\n",
    "Here we will suppose we have a data owner who wants a third party data scientist to train an AI model to detect spam from emails.\n",
    "\n",
    "The data owner will have to prepare the dataset first, and have it available in a PyTorch `DataSet` object before uploading it to the BastionLab server. We will download and unzip it by running the following code block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-12-12 17:02:54--  https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\n",
      "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
      "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 203415 (199K) [application/x-httpd-php]\n",
      "Saving to: ‘smsspamcollection.zip.2’\n",
      "\n",
      "smsspamcollection.z 100%[===================>] 198.65K   251KB/s    in 0.8s    \n",
      "\n",
      "2022-12-12 17:02:56 (251 KB/s) - ‘smsspamcollection.zip.2’ saved [203415/203415]\n",
      "\n",
      "Archive:  smsspamcollection.zip\n",
      "replace SMSSpamCollection? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n"
     ]
    }
   ],
   "source": [
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\n",
    "!unzip smsspamcollection.zip"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row represent a sample, the label comes first followed by a tab and the raw text:\n",
    "\n",
    "***# maybe let's display the first five lines?***\n",
    "\n",
    "```\n",
    "ham\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
    "ham\tOk lar... Joking wif u oni...\n",
    "spam\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
    "```\n",
    "\n",
    "***# why panda instead of polars??? hahaha***\n",
    "We first load the data from the file into a pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5574\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      0  Go until jurong point, crazy.. Available only ...\n",
       "1      0                    Ok lar... Joking wif u oni...\\n\n",
       "2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      0  U dun say so early hor... U c already then say...\n",
       "4      0  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = \"./SMSSpamCollection\"\n",
    "\n",
    "labels = []\n",
    "texts = []\n",
    "with open(file_path) as f:\n",
    "    for line in f.readlines():\n",
    "        split = line.split(\"\\t\")\n",
    "        labels.append(1 if split[0] == \"spam\" else 0)\n",
    "        texts.append(split[1])\n",
    "df = pd.DataFrame({\"label\": labels, \"text\": texts})\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then preprocess the data using a `DistilBertTokenizer` and we obtain tensors ready to be fed to the model:\n",
    "\n",
    "***# Why does that preprocesses the data? Also, this is a lot of code and very little explanations...***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "token_id = []\n",
    "attention_masks = []\n",
    "for sample in df.text.values:\n",
    "    encoding_dict = tokenizer.encode_plus(\n",
    "        sample,\n",
    "        add_special_tokens=True,\n",
    "        max_length=32,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_attention_mask=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    token_id.append(encoding_dict[\"input_ids\"])\n",
    "    attention_masks.append(encoding_dict[\"attention_mask\"])\n",
    "\n",
    "token_id = torch.cat(token_id, dim=0).to(dtype=torch.int64)\n",
    "attention_masks = torch.cat(attention_masks, dim=0).to(dtype=torch.int64)\n",
    "labels = torch.tensor(df.label.values, dtype=torch.int64)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will take only a subset of the dataset to make the training process faster. You can choose to take the whole dataset if you want.\n",
    "\n",
    "***# need more explanation as to what you are doing here and how?***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "test_ratio = 0.2\n",
    "limit = 64\n",
    "nb_samples = len(token_id)\n",
    "\n",
    "idx = np.arange(nb_samples)\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "train_idx = idx[int(test_ratio * nb_samples) :][:limit]\n",
    "test_idx = idx[: int(test_ratio * nb_samples)][:limit]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create two `TensorDataset` objects, that will be used to wrap our `Tensor` objects into a PyTorch `DataSet`\n",
    "\n",
    "***# same***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bastionlab.torch.utils import TensorDataset\n",
    "\n",
    "train_set = TensorDataset(\n",
    "    [token_id[train_idx], attention_masks[train_idx]], labels[train_idx]\n",
    ")\n",
    "\n",
    "test_set = TensorDataset(\n",
    "    [token_id[test_idx], attention_masks[test_idx]], labels[test_idx]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data scientist's side: preparing the model\n",
    "\n",
    "We now turn to preparing the DistilBERT language model. As Hugging Face's models typically have several outputs (logits, loss, etc.), we use BastionLab's utility wrapper for models with multiple outputs to select only the output that corresponds with the logits. In fact, BastionLab's server supports models with an arbitrtary number of inputs but only supports models with a single output.\n",
    "\n",
    "***# mysteryyy***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class MultipleOutputWrapper(nn.Module):\n",
    "    \"\"\"Utility wrapper to select one output of a model with multiple outputs.\n",
    "\n",
    "    Args:\n",
    "        module: A model with more than one output.\n",
    "        output: Index of the output to retain.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, module: nn.Module, output: int = 0) -> None:\n",
    "        super().__init__()\n",
    "        self.inner = module\n",
    "        self.output = output\n",
    "\n",
    "    def forward(self, *args, **kwargs) -> torch.Tensor:\n",
    "        output = self.inner.forward(*args, **kwargs)\n",
    "        return output[self.output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=2,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    "    torchscript=True,\n",
    ")\n",
    "model = MultipleOutputWrapper(\n",
    "    model, 0\n",
    ")  # This can be loaded from bastionlab.torch.utils"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online phase - dataset and model upload and training\n",
    "-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the dataset and model are prepared, we can go to the online phase where the dataset is uploaded securely in the enclave, and the model is sent there for training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data owner's side: uploading the dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will connect to the BastionLab Torch instance using our library. Once connected, we use the `RemoteDataset` method to upload the datasets inside, provide a name, and set a Differential Privacy budget. Here we put an arbitrary number, but as a rule of thumb, the DP budget should be much lower, such as 4 or 8. \n",
    "\n",
    "***# There is DP in BastionAI?***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sending SMSSpamCollection: 100%|████████████████████| 35.6k/35.6k [00:00<00:00, 8.86MB/s]\n",
      "Sending SMSSpamCollection (test): 100%|████████████████████| 35.6k/35.6k [00:00<00:00, 20.8MB/s]\n"
     ]
    }
   ],
   "source": [
    "from bastionlab import Connection\n",
    "\n",
    "# The Data Owner privately uploads their model online\n",
    "client = Connection(\"localhost\").client.torch\n",
    "\n",
    "remote_dataset = client.RemoteDataset(\n",
    "    train_set, test_set, name=\"SMSSpamCollection\", privacy_limit=1_000_000.0\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data scientist's side: uploading the model and trigger training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the Data Scientist side, we use the `list_remote_datasets` endpoint to get a list of available datasets on the server that we can use for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SMSSpamCollection (5cd871636638c4cde41c672d735e2ab0af1628edc43ccd4bcdbc126c35e95fa1): size=64, desc=N/A']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = Connection(\"localhost\").client.torch\n",
    "\n",
    "remote_datasets = client.list_remote_datasets()\n",
    "\n",
    "[str(ds) for ds in remote_datasets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see the dataset uploaded previously is available as a `RemoteDataset ` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bastionlab.torch.learner.RemoteDataset at 0x7f39bcb487c0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remote_datasets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This object is a pointer to the remote dataset uploaded previously. Note that this is a **pointer that contains only metadata and nothing else**. This way, the data scientist can play with remote datasets without users' data being exposed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use the `RemoteLearner` method to send the model to the server and to set all the necessary training parameters.\n",
    "\n",
    "To start training, we just call the `fit` method on the `RemoteLearner` object with an appropriate number of epochs and DP budget. \n",
    "Note that behind the scenes, a DP-SGD training loop will be used.\n",
    "\n",
    "We may finally retrieve a local copy of the trained model once the training is complete with the `get_model` method and test the model directly on the server with the `test` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sending DistilBERT: 100%|████████████████████| 268M/268M [00:05<00:00, 47.3MB/s] \n",
      "Epoch 1/2 - train:  12%|██▌                 | 4/32 [00:18<02:07,  4.54s/batch, cross_entropy=10.0000 (+/- 3656.3914)]  "
     ]
    }
   ],
   "source": [
    "from bastionlab.torch.optimizer_config import Adam\n",
    "\n",
    "# The Data Scientist discovers available datasets and uses one of them to train their model\n",
    "client = Connection(\"localhost\").client.torch\n",
    "remote_learner = client.RemoteLearner(\n",
    "    model,\n",
    "    remote_datasets[0],\n",
    "    max_batch_size=2,\n",
    "    loss=\"cross_entropy\",\n",
    "    optimizer=Adam(lr=5e-5),\n",
    "    model_name=\"DistilBERT\",\n",
    ")\n",
    "\n",
    "remote_learner.fit(nb_epochs=2, eps=6.0)\n",
    "remote_learner.test(metric=\"accuracy\")\n",
    "\n",
    "trained_model = remote_learner.get_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Jun 22 2022, 20:18:18) \n[GCC 9.4.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
